"""Test script for evaluating saved model checkpoints.

Usage:
python -m raf.segmentation.fl_experiments.test --checkpoint checkpoints/my_experiment/best.pth
python -m raf.segmentation.fl_experiments.test --checkpoint checkpoints/my_experiment/latest.pth --device cuda:1
"""

from __future__ import annotations

import argparse
import sys
from pathlib import Path
from typing import Dict, Any

import torch
from torch.utils.data import DataLoader
from easydict import EasyDict as edict
from omegaconf import DictConfig, OmegaConf

from .model_builder import build_model
from ..dataset.builder import DatasetBuilder
from .metrics import compute_miou
from .utils.seed import set_seed


def load_checkpoint(checkpoint_path: str | Path, device_id: int | str | None = None) -> tuple:
    """Load model checkpoint and return model, config, and metrics.
    
    Args:
        checkpoint_path: Path to checkpoint file
        device_id: Device to load model on
        
    Returns:
        (model, device, config, checkpoint_info)
    """
    checkpoint_path = Path(checkpoint_path)
    if not checkpoint_path.exists():
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
    
    print(f"üìÇ Loading checkpoint: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # Extract info
    config = checkpoint['config']
    num_classes = config.get('num_classes', 19)
    
    # Set seed for reproducibility
    seed = config.get('seed', 42)
    set_seed(seed)
    
    # Build model
    # Use same pretrained flag as training
    model_cfg = config.get("model", {})
    pretrained_flag = model_cfg.get("pretrained", False)
    model, device = build_model(num_classes, device_id, pretrained_encoder=pretrained_flag)
    
    # Load weights
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    checkpoint_info = {
        'epoch': checkpoint.get('epoch', checkpoint.get('round', 0)),  # Handle both centralized and federated
        'best_miou': checkpoint['best_miou'],
        'metrics': checkpoint['metrics']
    }
    
    print(f"‚úÖ Model loaded from epoch {checkpoint_info['epoch']}")
    # Handle both centralized and federated checkpoint formats
    if 'val_miou' in checkpoint_info['metrics']:
        miou_val = checkpoint_info['metrics']['val_miou']
    elif 'global_val_miou' in checkpoint_info['metrics']:
        miou_val = checkpoint_info['metrics']['global_val_miou']
    else:
        miou_val = 0.0
    print(f"üèÜ Checkpoint mIoU: {miou_val:.3f}")
    
    return model, device, config, checkpoint_info


def create_test_dataloader(config: Dict[str, Any]) -> DataLoader:
    """Create test dataloader from config."""
    # Use same batch size as training for consistency
    training_cfg = config.get("training", {})
    batch_size = training_cfg.get("batch_size", 1)
    crop_size = training_cfg.get("crop_size", [512, 512])
    
    # Handle ListConfig
    if hasattr(crop_size, '_content'):
        crop_size = list(crop_size)
    
    if isinstance(crop_size, list) and len(crop_size) >= 2:
        resolution = min(crop_size)
        crop_h, crop_w = crop_size[0], crop_size[1] 
    else:
        resolution = crop_size
        crop_h = crop_w = crop_size
        
    dataset_config = edict({
        'dataset_root': config['data']['data_root'],
        'train_split': 'train',
        'valid_split': 'val',
        'mode': 'gtFine',
        'target_type': 'semantic',
        'train_batch_size': batch_size,
        'valid_batch_size': batch_size,
        'num_workers': 4,
        'pin_memory': True,
        'transform_config': edict({
            'resolution': resolution,
            'crop_size': min(crop_h, crop_w),
            'brightness': 0.5,
            'contrast': 0.5,
            'saturation': 0.5,
        })
    })
    
    builder = DatasetBuilder(dataset_config)
    return builder.get_valid_dataloader()


@torch.no_grad()
def evaluate_model(model: torch.nn.Module, dataloader: DataLoader, 
                  device: torch.device, num_classes: int = 19) -> Dict[str, float]:
    """Evaluate model on test data."""
    model.eval()
    
    losses = []
    ious = []
    total_samples = 0
    
    print("üîç Running evaluation...")
    
    for i, (imgs, labels) in enumerate(dataloader):
        imgs, labels = imgs.to(device), labels.to(device)
        total_samples += imgs.size(0)  # Count actual samples in this batch
        outputs = model(pixel_values=imgs, labels=labels)
        losses.append(outputs.loss.item())
        
        # Compute mIoU
        miou = compute_miou(outputs.logits, labels, num_classes=num_classes)
        ious.append(miou)
        
        if (i + 1) % 100 == 0:
            print(f"  Processed {i + 1}/{len(dataloader)} batches...")
    
    results = {
        "test_loss": float(sum(losses) / len(losses)),
        "test_miou": float(sum(ious) / len(ious)),
        "num_batches": len(dataloader),
        "num_samples": total_samples  # Total images processed
    }
    
    return results


def main() -> None:
    parser = argparse.ArgumentParser(description="Test trained SegFormer model")
    parser.add_argument("--checkpoint", "-c", type=str, required=True,
                       help="Path to checkpoint file")
    parser.add_argument("--device", "-d", type=str, default=None,
                       help="Device to run on (e.g., cuda:0, cpu)")
    parser.add_argument("--data_root", type=str, default=None,
                       help="Override data root path")
    parser.add_argument("--resolution", type=int, nargs=2, default=None,
                       help="Override test resolution [height, width] (e.g., --resolution 512 1024)")
    
    args = parser.parse_args()
    
    try:
        # Load checkpoint
        model, device, config, checkpoint_info = load_checkpoint(args.checkpoint, args.device)
        
        # Override data root if provided
        if args.data_root:
            config['data']['data_root'] = args.data_root
            print(f"üìÅ Using data root: {args.data_root}")
        
        # Override resolution if provided
        if args.resolution:
            config['training']['crop_size'] = args.resolution
            print(f"üñºÔ∏è  Using test resolution: {args.resolution[0]}√ó{args.resolution[1]}")
        
        # Create test dataloader
        test_loader = create_test_dataloader(config)
        
        # Run evaluation
        print("-" * 60)
        results = evaluate_model(model, test_loader, device, config.get('num_classes', 19))
        
        # Print results
        print("-" * 60)
        print("üìä Test Results:")
        print(f"   Test Loss: {results['test_loss']:.4f}")
        print(f"   Test mIoU: {results['test_miou']:.3f}")
        print(f"   Samples: {results['num_samples']} (in {results['num_batches']} batches)")
        print("-" * 60)
        
        # Handle both centralized ('val_miou') and federated ('global_val_miou') formats
        if 'val_miou' in checkpoint_info['metrics']:
            val_miou = checkpoint_info['metrics']['val_miou']
        elif 'global_val_miou' in checkpoint_info['metrics']:
            val_miou = checkpoint_info['metrics']['global_val_miou']
        else:
            val_miou = None
        
        if val_miou is not None:
            print(f"üìà Comparison:")
            print(f"   Validation mIoU: {val_miou:.3f}")
            print(f"   Test mIoU: {results['test_miou']:.3f}")
            print(f"   Difference: {results['test_miou'] - val_miou:+.3f}")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 